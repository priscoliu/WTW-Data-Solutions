{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02_silver_notebook_eclipse\n",
                "\n",
                "**Purpose**: Migrate Eclipse Data from Alteryx ETL Process to Fabric PySpark.\n",
                "\n",
                "**Sources**:\n",
                "- `APAC_CRM_Analytics_LH.src_eclipse_crb` (Bronze — Asia Stream)\n",
                "- `APAC_CRM_Analytics_LH.src_eclipse_london` (Bronze — London Stream)\n",
                "\n",
                "**Output**: `APAC_Reporting_LH.clean_eclipse_chloe` (Silver)\n",
                "\n",
                "**Reference Tables** (all in `APAC_CRM_Analytics_LH`):\n",
                "- `ref_Chloe_insurer_mapping` — Insurer mapping (shared)\n",
                "- `ref_Chloe_eclipse_product_mapping` — Product / LOB mapping\n",
                "- `ref_Chloe_Transaction_type_mapping` — Transaction type mapping\n",
                "\n",
                "**Alteryx Tool Mapping**:\n",
                "| Cell | Alteryx Tools |\n",
                "| :--- | :--- |\n",
                "| Cell 2 | Input Asia (src_eclipse_crb), Input London (src_eclipse_london), stream filters |\n",
                "| Cell 3 | Formula Asia (Tool 93), Formula London (Tool 134) |\n",
                "| Cell 4 | Union, Select (Tool 131 — renames), Cleanse |\n",
                "| Cell 5 | Insurer Join (Tools 101/102), Product Join (Tools 139/140), TransType Join, Final Select |\n",
                "| Cell 6 | Output to Silver |"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "# =============================================================================\n",
                "# Cell 1: Setup & Configuration\n",
                "# =============================================================================\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.types import StringType, DoubleType, DateType, IntegerType, LongType, FloatType, DecimalType\n",
                "from pyspark.sql.utils import AnalysisException\n",
                "import re\n",
                "\n",
                "# Lakehouse\n",
                "BRONZE_LH = \"APAC_CRM_Analytics_LH\"\n",
                "SILVER_LH = \"APAC_Reporting_LH\"\n",
                "\n",
                "# Tables\n",
                "SOURCE_ASIA = f\"{BRONZE_LH}.src_eclipse_crb\"\n",
                "SOURCE_LONDON = f\"{BRONZE_LH}.src_eclipse_london\"\n",
                "TARGET_TABLE = f\"{SILVER_LH}.clean_eclipse_chloe\"\n",
                "\n",
                "# Reference tables\n",
                "REF_INSURER = f\"{BRONZE_LH}.ref_Chloe_insurer_mapping\"\n",
                "REF_PRODUCT = f\"{BRONZE_LH}.ref_Chloe_eclipse_product_mapping\"\n",
                "REF_TRANS = f\"{BRONZE_LH}.ref_Chloe_Transaction_type_mapping\""
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "# =============================================================================\n",
                "# Cell 2: Load Bronze Data\n",
                "# =============================================================================\n",
                "\n",
                "# --- 1. Asia Stream (Eclipse Recurring Report) ---\n",
                "try:\n",
                "    df_asia = spark.sql(f\"SELECT * FROM {SOURCE_ASIA}\")\n",
                "    df_asia = df_asia.withColumn(\"Origin_Stream\", F.lit(\"Asia\")) \\\n",
                "        .filter(~F.col(\"LegalEntity\").isin(\"PT. Willis Reinsurance Brokers Indonesia\", \"Willis Towers Watson Taiwan Limited\"))\n",
                "\n",
                "    print(\"=== ASIA SOURCE SCHEMA ===\")\n",
                "    df_asia.printSchema()\n",
                "    print(f\"\\n=== ASIA ROW COUNT: {df_asia.count()} ===\")\n",
                "\n",
                "except AnalysisException:\n",
                "    raise Exception(f\"ERROR: Source table {SOURCE_ASIA} not found.\")\n",
                "\n",
                "# --- 2. London Stream (Combined MIR files) ---\n",
                "try:\n",
                "    df_london = spark.sql(f\"SELECT * FROM {SOURCE_LONDON}\")\n",
                "    df_london = df_london.withColumn(\"Origin_Stream\", F.lit(\"London\"))\n",
                "\n",
                "    # London Filters (Source.Name based logic)\n",
                "    exclude_12046 = [\"China\", \"Hong Kong\", \"India\", \"Japan\", \"Malaysia\", \"New Zealand\", \"Philippines\", \"Republic of Korea\", \"Singapore\", \"Taiwan\", \"Thailand\"]\n",
                "    exclude_12047 = [\"Bahrain\", \"Cyprus\", \"Georgia\", \"Jordan\", \"Kazakhstan\", \"Kuwait\", \"Oman\", \"Qatar\", \"Turkey\", \"United Arab Emirates\"]\n",
                "\n",
                "    # Condition: (Contains 12046 AND Country in Exclude List A) OR (Contains 12047 AND Country NOT in Exclude List B)\n",
                "    cond_exclude = (\n",
                "        (F.col(\"`Source.Name`\").contains(\"12046\") & F.col(\"UWCountry\").isin(exclude_12046)) |\n",
                "        (F.col(\"`Source.Name`\").contains(\"12047\") & ~F.col(\"UWCountry\").isin(exclude_12047))\n",
                "    )\n",
                "    df_london = df_london.filter(~cond_exclude)\n",
                "\n",
                "    print(\"=== LONDON SOURCE SCHEMA ===\")\n",
                "    df_london.printSchema()\n",
                "    print(f\"\\n=== LONDON ROW COUNT: {df_london.count()} ===\")\n",
                "\n",
                "except AnalysisException:\n",
                "    raise Exception(f\"ERROR: Source table {SOURCE_LONDON} not found.\")"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "# =============================================================================\n",
                "# Cell 3: Transformation Logic (Replicating Alteryx Tools 93 & 134)\n",
                "#   - Asia: REVENUE COUNTRY logic, BuTeam, FinalDate, PolicyDescription\n",
                "#   - London: hardcoded REVENUE COUNTRY, FinalDate, PolicyDescription\n",
                "# =============================================================================\n",
                "\n",
                "# --- ASIA LOGIC (Alteryx Tool 93) ---\n",
                "# REVENUE COUNTRY Logic:\n",
                "cond_rev_country_asia = (\n",
                "    F.when(F.col(\"BuTeam\") == \"Retail+Commercial\", \"Singapore\")\n",
                "    .when(F.col(\"BuTeam\") == \"Retail+Construction\", \"Singapore\")\n",
                "    .when(F.col(\"BuTeam\").contains(\"Retail+Client Service Team\"), \"Singapore\")\n",
                "    .when(F.col(\"LegalEntity\").contains(\"Hong Kong\"), \"Hong Kong\")\n",
                "    .when(F.col(\"LegalEntity\").contains(\"Philippines\"), \"Philippines\")\n",
                "    .otherwise(\"Regional Specialism\")\n",
                ")\n",
                "\n",
                "df_asia_trans = (df_asia\n",
                "    .withColumn(\"DataSource\", F.lit(\"Eclipse\"))\n",
                "    .withColumn(\"ClientId\", F.coalesce(F.col(\"Willis Party ID\"), F.col(\"InsuredID\")))\n",
                "    .withColumn(\"BuTeam\", F.concat(F.col(\"BusinessUnit\"), F.lit(\"+\"), F.col(\"Team\")))\n",
                "    .withColumn(\"RevenueCountry\", cond_rev_country_asia)\n",
                "    .withColumn(\"ProductsToBeMapped\", F.upper(F.concat(F.col(\"BusinessUnit\"), F.lit(\"+\"), F.col(\"Team\"), F.lit(\"+\"), F.col(\"ClassOfBusiness\"))))\n",
                "    .withColumn(\"UwClean\", F.upper(F.trim(F.col(\"UW\"))))\n",
                "    .withColumn(\"FinalDate\", F.when(F.col(\"TransRef\") == \"INVOICE DATE\", F.col(\"CreatedDate\")).otherwise(F.col(\"InceptionDate\")))\n",
                "    .withColumn(\"PolicyDescription\", F.when(F.col(\"BusinessType\") == \"Reinsurance\", F.lit(\"Reinsurance\")).otherwise(F.lit(\"null\")))\n",
                "    .withColumn(\"ReinsuranceDescription\", F.lit(\"null\"))\n",
                ")\n",
                "\n",
                "# --- LONDON LOGIC (Alteryx Tool 134) ---\n",
                "df_london_trans = (df_london\n",
                "    .withColumn(\"DataSource\", F.lit(\"Eclipse\"))\n",
                "    .withColumn(\"ClientId\", F.coalesce(F.col(\"Willis Party ID\"), F.col(\"InsuredID\")))\n",
                "    .withColumn(\"RevenueCountry\", F.lit(\"London Placements\"))\n",
                "    .withColumn(\"ProductsToBeMapped\", F.trim(F.upper(F.col(\"ClassOfBusiness\"))))\n",
                "    .withColumn(\"UwClean\", F.upper(F.trim(F.col(\"UW\"))))\n",
                "    .withColumn(\"FinalDate\", F.when(F.col(\"TransRef\") == \"INVOICE DATE\", F.col(\"CreatedDate\")).otherwise(F.col(\"InceptionDate\")))\n",
                "    .withColumn(\"PolicyDescription\", F.when(F.col(\"BusinessType\") == \"Reinsurance\", F.lit(\"Reinsurance\")).otherwise(F.lit(\"null\")))\n",
                "    .withColumn(\"ReinsuranceDescription\", F.lit(\"null\"))\n",
                ")\n",
                "\n",
                "print(\"=== SCHEMA AFTER FORMULAS ===\")\n",
                "df_asia_trans.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "# =============================================================================\n",
                "# Cell 4: Union & Unification (Replicating Alteryx Union + Select Tool 131)\n",
                "# =============================================================================\n",
                "\n",
                "df_unified = df_asia_trans.unionByName(df_london_trans, allowMissingColumns=True)\n",
                "\n",
                "# Common Final Transform (Alteryx Select Tool 131 Rename)\n",
                "# FIX: Drop existing 'Department' column if it exists to avoid ambiguity when renaming 'Team'\n",
                "for c in [col_name for col_name in df_unified.columns if col_name.lower() == \"department\"]:\n",
                "    df_unified = df_unified.drop(c)\n",
                "\n",
                "df_renamed = (df_unified\n",
                "    .withColumnRenamed(\"Team\", \"Department\")\n",
                "    .withColumnRenamed(\"PolicyRef\", \"InvoicePolicyNumber\")\n",
                "    .withColumnRenamed(\"Account Handler\", \"AccountHandler\")\n",
                "    .withColumnRenamed(\"CreatedDate\", \"InvoiceDate\")\n",
                "    .withColumnRenamed(\"Insured\", \"ClientName\")\n",
                "    .withColumnRenamed(\"UW\", \"InsurerName\")\n",
                "    .withColumnRenamed(\"UWCountry\", \"InsurerCountry\")\n",
                "    .withColumnRenamed(\"GrossBkgeUSDPlan\", \"BrokerageUsd\")\n",
                "    .withColumnRenamed(\"GrossPremNonTtyUSDPlan\", \"PremiumUsd\")\n",
                "    .withColumnRenamed(\"ClassOfBusiness\", \"SystemProductId\")\n",
                "    .withColumnRenamed(\"Willis Party ID\", \"PartyIdWtw\")\n",
                "    .withColumnRenamed(\"Dun and Bradstreet No\", \"DunsNumber\")\n",
                "    .withColumnRenamed(\"Revenue Type\", \"TransactionType\")\n",
                ")\n",
                "\n",
                "print(\"=== SCHEMA AFTER UNION & RENAME ===\")\n",
                "df_renamed.printSchema()\n",
                "print(f\"\\n=== UNIFIED ROW COUNT: {df_renamed.count()} ===\")"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "# =============================================================================\n",
                "# Cell 5: Reference Joins + Final Select\n",
                "# =============================================================================\n",
                "\n",
                "# --- Load reference tables ---\n",
                "try:\n",
                "    df_insurer = spark.sql(f\"SELECT * FROM {REF_INSURER}\")\n",
                "    df_product = spark.sql(f\"SELECT * FROM {REF_PRODUCT}\")\n",
                "    df_trans = spark.sql(f\"SELECT * FROM {REF_TRANS}\")\n",
                "except AnalysisException as e:\n",
                "    print(f\"WARNING: Reference table not found — {e}\")\n",
                "    raise\n",
                "\n",
                "# --- Join 1: Insurer Mapping (Tools 101/102) ---\n",
                "# Key: InsurerName == Insurer\n",
                "# Brings in: MAPPED_INSURER, Lloyd's Asia or Lloyd's London\n",
                "df_insurer_ref = df_insurer.select(\n",
                "    F.upper(F.trim(F.col(\"Insurer\"))).alias(\"_insurer_join_key\"),\n",
                "    F.col(\"MAPPED_INSURER\"),\n",
                "    F.col(\"`Lloyd's Asia or Lloyd's London`\")\n",
                ")\n",
                "\n",
                "df = df_renamed.join(\n",
                "    df_insurer_ref,\n",
                "    F.trim(F.upper(df_renamed[\"InsurerName\"])) == df_insurer_ref[\"_insurer_join_key\"],\n",
                "    \"left\"\n",
                ").drop(\"_insurer_join_key\")\n",
                "\n",
                "# --- Join 2: Product Mapping (Tools 139/140) ---\n",
                "# Key: ProductsToBeMapped == Filter Fac Product\n",
                "# Brings in: Level 2 Mapping, GLOBS, GLOBS SPLIT P&C\n",
                "df_product_ref = df_product.select(\n",
                "    F.upper(F.trim(F.col(\"`Filter Fac Product`\"))).alias(\"_product_join_key\"),\n",
                "    F.col(\"`Level 2 Mapping`\"),\n",
                "    F.col(\"GLOBS\"),\n",
                "    F.col(\"`GLOBS SPLIT P&C`\")\n",
                ")\n",
                "\n",
                "df = df.join(\n",
                "    df_product_ref,\n",
                "    F.trim(F.upper(df[\"ProductsToBeMapped\"])) == df_product_ref[\"_product_join_key\"],\n",
                "    \"left\"\n",
                ").drop(\"_product_join_key\")\n",
                "\n",
                "# --- Join 3: Transaction Type ---\n",
                "# Key: TransactionType == TransType\n",
                "df_trans_ref = df_trans.select(\n",
                "    F.upper(F.trim(F.col(\"TransType\"))).alias(\"_trans_join_key\"),\n",
                "    F.col(\"TransType\")\n",
                ")\n",
                "\n",
                "df = df.join(\n",
                "    df_trans_ref,\n",
                "    F.trim(F.upper(df[\"TransactionType\"])) == df_trans_ref[\"_trans_join_key\"],\n",
                "    \"left\"\n",
                ").drop(\"_trans_join_key\")\n",
                "\n",
                "# --- Final Select: PascalCase aliases ---\n",
                "df_final = df.select(\n",
                "    F.col(\"BrokerageUsd\").cast(DoubleType()).alias(\"BrokerageUsd\"),\n",
                "    F.col(\"BusinessType\").cast(StringType()).alias(\"BusinessType\"),\n",
                "    F.col(\"ClientId\").cast(StringType()).alias(\"ClientIdWtw\"),\n",
                "    F.col(\"ClientName\").cast(StringType()).alias(\"ClientName\"),\n",
                "    F.col(\"DataSource\").cast(StringType()).alias(\"DataSource\"),\n",
                "    F.col(\"Department\").cast(StringType()).alias(\"Department\"),\n",
                "    F.col(\"DunsNumber\").cast(StringType()).alias(\"DunsNumber\"),\n",
                "    F.col(\"InvoiceDate\").cast(DateType()).alias(\"InvoiceDate\"),\n",
                "    F.col(\"ExpiryDate\").cast(DateType()).alias(\"ExpiryDate\"),\n",
                "    F.col(\"FinalDate\").cast(DateType()).alias(\"FinalDate\"),\n",
                "    F.col(\"GLOBS\").cast(StringType()).alias(\"Globs\"),\n",
                "    F.col(\"`GLOBS SPLIT P&C`\").cast(StringType()).alias(\"GlobsSplitPc\"),\n",
                "    F.col(\"InsurerCountry\").cast(StringType()).alias(\"InsurerCountry\"),\n",
                "    F.col(\"InsurerName\").cast(StringType()).alias(\"InsurerName\"),\n",
                "    F.col(\"`Lloyd's Asia or Lloyd's London`\").cast(StringType()).alias(\"Lloyds\"),\n",
                "    F.col(\"`Level 2 Mapping`\").cast(StringType()).alias(\"SubProductClass\"),\n",
                "    F.col(\"MAPPED_INSURER\").cast(StringType()).alias(\"InsurerMapping\"),\n",
                "    F.col(\"PartyIdWtw\").cast(StringType()).alias(\"PartyIdWtw\"),\n",
                "    F.col(\"PolicyDescription\").cast(StringType()).alias(\"PolicyDescription\"),\n",
                "    F.col(\"SystemProductId\").cast(StringType()).alias(\"SystemProductId\"),\n",
                "    F.col(\"InvoicePolicyNumber\").cast(StringType()).alias(\"InvoicePolicyNumber\"),\n",
                "    F.col(\"PremiumUsd\").cast(DoubleType()).alias(\"PremiumUsd\"),\n",
                "    F.col(\"ReinsuranceDescription\").cast(StringType()).alias(\"ReinsuranceDescription\"),\n",
                "    F.col(\"RevenueCountry\").cast(StringType()).alias(\"RevenueCountry\"),\n",
                "    F.col(\"AccountHandler\").cast(StringType()).alias(\"AccountHandler\"),\n",
                "    F.col(\"InceptionDate\").cast(DateType()).alias(\"InceptionDate\"),\n",
                "    F.col(\"InsuredID\").cast(StringType()).alias(\"SystemId\"),\n",
                "    F.col(\"TransactionType\").cast(StringType()).alias(\"TransactionType\")\n",
                ")\n",
                "\n",
                "print(\"=== FINAL SCHEMA ===\")\n",
                "df_final.printSchema()\n",
                "print(f\"\\n=== FINAL ROW COUNT: {df_final.count()} ===\")\n",
                "print(\"\\n=== FINAL SAMPLE (first 5 rows) ===\")\n",
                "display(df_final.limit(5))"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "# =============================================================================\n",
                "# Cell 6: Write to Silver\n",
                "# =============================================================================\n",
                "\n",
                "# --- Standardize column order across all silver notebooks ---\n",
                "STANDARD_COLUMNS = [\n",
                "    \"AccountHandler\", \"BrokerageUsd\", \"BusinessType\", \"ClientIdWtw\", \"ClientName\",\n",
                "    \"DataSource\", \"Department\", \"DunsNumber\", \"ExpiryDate\", \"FinalDate\",\n",
                "    \"Globs\", \"GlobsSplitPc\", \"InceptionDate\", \"InsurerCountry\", \"InsurerMapping\",\n",
                "    \"InsurerName\", \"InvoiceDate\", \"InvoicePolicyNumber\", \"Lloyds\", \"PartyIdWtw\",\n",
                "    \"PolicyDescription\", \"PremiumUsd\", \"ReinsuranceDescription\", \"RevenueCountry\",\n",
                "    \"SubProductClass\", \"SystemId\", \"SystemProductId\", \"TransactionType\"\n",
                "]\n",
                "df_final = df_final.select(*STANDARD_COLUMNS)\n",
                "\n",
                "print(f\"Writing to {TARGET_TABLE}...\")\n",
                "df_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(TARGET_TABLE)\n",
                "\n",
                "print(f\"Success. Rows written: {spark.table(TARGET_TABLE).count()}\")\n",
                "print(f\"Columns: {len(spark.table(TARGET_TABLE).columns)}\")\n",
                "display(spark.table(TARGET_TABLE).limit(5))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}