{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Eclipse Silver Notebook\n",
                "Migration of Alteryx Eclipse workflow to Fabric PySpark.\n",
                "- **Asia Stream**: `src_eclipse_crb`\n",
                "- **London Stream**: `src_eclipse_london`\n",
                "- **Output**: `clean_eclipse_chloe` in `APAC_Reporting_LH`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Setup & Configuration\n",
                "# -----------------------------\n",
                "from pyspark.sql.functions import col, when, trim, upper, lit, current_date, coalesce, isnan, count, concat, expr, size, collect_set, regexp_replace\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql.types import IntegerType, DoubleType, DateType, StringType, LongType, FloatType, DecimalType\n",
                "from pyspark.sql.utils import AnalysisException\n",
                "import re\n",
                "\n",
                "# Helper: Cleanse DataFrame\n",
                "print(\"Applying cleansing transformations...\")\n",
                "\n",
                "def cleanse_dataframe(df):\n",
                "    print(\"Applying cleansing transformations...\")\n",
                "    \n",
                "    string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
                "    numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, (IntegerType, DoubleType, LongType, FloatType, DecimalType))]\n",
                "\n",
                "    cleansed_df = df\n",
                "\n",
                "    for col_name in numeric_cols:\n",
                "        cleansed_df = cleansed_df.withColumn(\n",
                "            col_name,\n",
                "            F.coalesce(F.col(f\"`{col_name}`\"), F.lit(0))\n",
                "        )\n",
                "\n",
                "    for col_name in string_cols:\n",
                "        cleansed_df = cleansed_df.withColumn(\n",
                "            col_name,\n",
                "            F.upper(\n",
                "                F.regexp_replace(\n",
                "                    F.regexp_replace(\n",
                "                        F.trim(\n",
                "                            F.coalesce(F.col(f\"`{col_name}`\"), F.lit(''))\n",
                "                        ),\n",
                "                        r'[\\t\\n\\r]', ''\n",
                "                    ),\n",
                "                    r'\\s+', ' '\n",
                "                )\n",
                "            )\n",
                "        )\n",
                "\n",
                "    if 'Likelihood_of_Win' in cleansed_df.columns:\n",
                "        cleansed_df = cleansed_df.withColumn(\n",
                "            \"Likelihood_of_Win\",\n",
                "            F.regexp_replace(F.col(\"`Likelihood_of_Win`\"), \"%\", \"\").cast(DoubleType())\n",
                "        )\n",
                "\n",
                "    print(\"Cleansing finished.\")\n",
                "    return cleansed_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Load Bronze Data\n",
                "# ------------------------\n",
                "# 1. Asia Stream (Eclipse Recurring Report)\n",
                "try:\n",
                "    # UPDATED: Using SQL Endpoint access as per standard\n",
                "    df_asia = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.src_eclipse_crb\")\n",
                "    df_asia = df_asia.withColumn(\"Origin_Stream\", lit(\"Asia\")) \\\n",
                "        .filter(~col(\"LegalEntity\").isin(\"PT. Willis Reinsurance Brokers Indonesia\", \"Willis Towers Watson Taiwan Limited\"))\n",
                "except AnalysisException:\n",
                "    print(\"WARNING: src_eclipse_crb not found in APAC_CRM_Analytics_LH. Creating empty dummy DF.\")\n",
                "    df_asia = spark.createDataFrame([], schema=\"LegalEntity string, BusinessUnit string, Team string, ClassOfBusiness string, UW string\")\n",
                "\n",
                "# 2. London Stream (Combined MIR files)\n",
                "try:\n",
                "    # UPDATED: Using SQL Endpoint access as per standard\n",
                "    df_london = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.src_eclipse_london\")\n",
                "    df_london = df_london.withColumn(\"Origin_Stream\", lit(\"London\"))\n",
                "\n",
                "    # London Filters (Source.Name based logic)\n",
                "    # Lists for exclusion based on filename context\n",
                "    exclude_12046 = [\"China\", \"Hong Kong\", \"India\", \"Japan\", \"Malaysia\", \"New Zealand\", \"Philippines\", \"Republic of Korea\", \"Singapore\", \"Taiwan\", \"Thailand\"]\n",
                "    exclude_12047 = [\"Bahrain\", \"Cyprus\", \"Georgia\", \"Jordan\", \"Kazakhstan\", \"Kuwait\", \"Oman\", \"Qatar\", \"Turkey\", \"United Arab Emirates\"]\n",
                "\n",
                "    # Condition: (Contains 12046 AND Country in Exclude List A) OR (Contains 12047 AND Country NOT in Exclude List B)\n",
                "    # Using backticks for Source.Name to handle the dot\n",
                "    cond_exclude = (\n",
                "        (col(\"`Source.Name`\").contains(\"12046\") & col(\"UWCountry\").isin(exclude_12046)) | \n",
                "        (col(\"`Source.Name`\").contains(\"12047\") & ~col(\"UWCountry\").isin(exclude_12047))\n",
                "    )\n",
                "    df_london = df_london.filter(~cond_exclude)\n",
                "except AnalysisException:\n",
                "    print(\"WARNING: src_eclipse_london not found in APAC_CRM_Analytics_LH. Creating empty dummy DF.\")\n",
                "    df_london = spark.createDataFrame([], schema=\"LegalEntity string, BusinessUnit string, Team string, ClassOfBusiness string, UW string\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Transformation Logic (Replicating Alteryx Tools 93 & 134)\n",
                "# -----------------------------------------------------------------\n",
                "\n",
                "# --- ASIA LOGIC (Alteryx Tool 93) ---\n",
                "# REVENUE COUNTRY Logic:\n",
                "cond_rev_country_asia = (\n",
                "    when(col(\"BuTeam\") == \"Retail+Commercial\", \"Singapore\")\n",
                "    .when(col(\"BuTeam\") == \"Retail+Construction\", \"Singapore\")\n",
                "    .when(col(\"BuTeam\").contains(\"Retail+Client Service Team\"), \"Singapore\")\n",
                "    .when(col(\"LegalEntity\").contains(\"Hong Kong\"), \"Hong Kong\")\n",
                "    .when(col(\"LegalEntity\").contains(\"Philippines\"), \"Philippines\")\n",
                "    .otherwise(\"Regional Specialism\")\n",
                ")\n",
                "\n",
                "df_asia_trans = df_asia \\\n",
                "    .withColumn(\"DataSource\", lit(\"Eclipse\")) \\\n",
                "    .withColumn(\"ClientId\", coalesce(col(\"Willis Party ID\"), col(\"InsuredID\"))) \\\n",
                "    .withColumn(\"BuTeam\", concat(col(\"BusinessUnit\"), lit(\"+\"), col(\"Team\"))) \\\n",
                "    .withColumn(\"RevenueCountry\", cond_rev_country_asia) \\\n",
                "    .withColumn(\"ProductsToBeMapped\", upper(concat(col(\"BusinessUnit\"), lit(\"+\"), col(\"Team\"), lit(\"+\"), col(\"ClassOfBusiness\")))) \\\n",
                "    .withColumn(\"UwClean\", upper(trim(col(\"UW\")))) \\\n",
                "    .withColumn(\"FinalDate\", when(col(\"TransRef\") == \"INVOICE DATE\", col(\"CreatedDate\")).otherwise(col(\"InceptionDate\"))) \\\n",
                "    .withColumn(\"PolicyDescription\", when(col(\"BusinessType\") == \"Reinsurance\", lit(\"Reinsurance\")).otherwise(lit(\"null\"))) \\\n",
                "    .withColumn(\"ReinsuranceDescription\", lit(\"null\"))\n",
                "\n",
                "# --- LONDON LOGIC (Alteryx Tool 134) ---\n",
                "df_london_trans = df_london \\\n",
                "    .withColumn(\"DataSource\", lit(\"Eclipse\")) \\\n",
                "    .withColumn(\"ClientId\", coalesce(col(\"Willis Party ID\"), col(\"InsuredID\"))) \\\n",
                "    .withColumn(\"RevenueCountry\", lit(\"London Placements\")) \\\n",
                "    .withColumn(\"ProductsToBeMapped\", trim(upper(col(\"ClassOfBusiness\")))) \\\n",
                "    .withColumn(\"UwClean\", upper(trim(col(\"UW\")))) \\\n",
                "    .withColumn(\"FinalDate\", when(col(\"TransRef\") == \"INVOICE DATE\", col(\"CreatedDate\")).otherwise(col(\"InceptionDate\"))) \\\n",
                "    .withColumn(\"PolicyDescription\", when(col(\"BusinessType\") == \"Reinsurance\", lit(\"Reinsurance\")).otherwise(lit(\"null\"))) \\\n",
                "    .withColumn(\"ReinsuranceDescription\", lit(\"null\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Union & Unification (Replicating Alteryx Union)\n",
                "# ------------------------------------------------------\n",
                "# Standardize columns before union if needed, or rely on allowMissingColumns\n",
                "df_unified = df_asia_trans.unionByName(df_london_trans, allowMissingColumns=True)\n",
                "\n",
                "# Common Final Transform (Alteryx Select Tool 131 Rename)\n",
                "# We rename columns first to matches the target schema before cleaning and joining\n",
                "\n",
                "# FIX: Drop existing 'Department' column if it exists to avoid ambiguity when renaming 'Team'\n",
                "for c in [col_name for col_name in df_unified.columns if col_name.lower() == \"department\"]:\n",
                "    df_unified = df_unified.drop(c)\n",
                "\n",
                "df_renamed = df_unified \\\n",
                "    .withColumnRenamed(\"Team\", \"Department\") \\\n",
                "    .withColumnRenamed(\"PolicyRef\", \"InvoicePolicyNumber\") \\\n",
                "    .withColumnRenamed(\"BusinessType\", \"BusinessType\") \\\n",
                "    .withColumnRenamed(\"InceptionDate\", \"InceptionDate\") \\\n",
                "    .withColumnRenamed(\"ExpiryDate\", \"ExpiryDate\") \\\n",
                "    .withColumnRenamed(\"Account Handler\", \"AccountHandler\") \\\n",
                "    .withColumnRenamed(\"CreatedDate\", \"InvoiceDate\") \\\n",
                "    .withColumnRenamed(\"Insured\", \"ClientName\") \\\n",
                "    .withColumnRenamed(\"UW\", \"InsurerName\") \\\n",
                "    .withColumnRenamed(\"UWCountry\", \"InsurerCountry\") \\\n",
                "    .withColumnRenamed(\"GrossBkgeUSDPlan\", \"BrokerageUsd\") \\\n",
                "    .withColumnRenamed(\"GrossPremNonTtyUSDPlan\", \"PremiumUsd\") \\\n",
                "    .withColumnRenamed(\"ClassOfBusiness\", \"SystemProductId\") \\\n",
                "    .withColumnRenamed(\"Willis Party ID\", \"PartyIdWtw\") \\\n",
                "    .withColumnRenamed(\"Dun and Bradstreet No\", \"DunsNumber\") \\\n",
                "    .withColumnRenamed(\"Revenue Type\", \"TransactionType\")\n",
                "\n",
                "# --- APPLY CLEANSING TRANSFORMATIONS ---\n",
                "# Apply the robust cleansing function here (Replaces nulls, trims, uppercases strings)\n",
                "df_cleansed = cleanse_dataframe(df_renamed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Reference Joins (Replicating Tools 101, 102, 139, 140)\n",
                "# --------------------------------------------------------------\n",
                "\n",
                "# 1. Load Reference Tables\n",
                "try:\n",
                "    # UPDATED: Using SQL Endpoint access as per standard, correct table names\n",
                "    ref_insurer = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.ref_Chloe_insurer_mapping\")\n",
                "    ref_product = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.ref_Chloe_eclipse_product_mapping\")\n",
                "    ref_trans = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.ref_Chloe_Transaction_type_mapping\")\n",
                "except:\n",
                "    print(\"Reference tables missing (Insurer/Product/TransType). Skipping specific joins (Mock Mode).\")\n",
                "    ref_insurer = None\n",
                "    ref_product = None\n",
                "    ref_trans = None\n",
                "\n",
                "\n",
                "# 2. Join Insurer\n",
                "if ref_insurer:\n",
                "    # Join on InsurerName == Insurer (Robust: Trim + Upper)\n",
                "    df_joined_1 = df_cleansed.join(\n",
                "        ref_insurer, \n",
                "        F.trim(F.upper(df_cleansed[\"InsurerName\"])) == F.trim(F.upper(ref_insurer[\"Insurer\"])), \n",
                "        \"left\"\n",
                "    )\n",
                "else:\n",
                "    df_joined_1 = df_cleansed\n",
                "\n",
                "# 3. Join Product\n",
                "if ref_product:\n",
                "    # Join on ProductsToBeMapped == Filter Fac Product (Robust: Trim + Upper)\n",
                "    df_joined_2 = df_joined_1.join(\n",
                "        ref_product, \n",
                "        F.trim(F.upper(df_joined_1[\"ProductsToBeMapped\"])) == F.trim(F.upper(ref_product[\"Filter Fac Product\"])), \n",
                "        \"left\"\n",
                "    )\n",
                "else:\n",
                "    df_joined_2 = df_joined_1\n",
                "\n",
                "# 4. Join Transaction Type\n",
                "if ref_trans:\n",
                "    # Join on TransType == TransType (Robust: Trim + Upper)\n",
                "    df_joined_3 = df_joined_2.join(\n",
                "        ref_trans, \n",
                "        F.trim(F.upper(df_joined_2[\"TransType\"])) == F.trim(F.upper(ref_trans[\"TransType\"])), \n",
                "        \"left\"\n",
                "    )\n",
                "else:\n",
                "    df_joined_3 = df_joined_2\n",
                "\n",
                "# Set final dataframe for output (with explicit data types)\n",
                "df_enriched = df_joined_3.select(\n",
                "    col(\"Department\").cast(StringType()),\n",
                "    col(\"InvoicePolicyNumber\").cast(StringType()),\n",
                "    col(\"BusinessType\").cast(StringType()),\n",
                "    col(\"InceptionDate\").cast(DateType()),\n",
                "    col(\"ExpiryDate\").cast(DateType()),\n",
                "    col(\"AccountHandler\").cast(StringType()),\n",
                "    col(\"InvoiceDate\").cast(DateType()),\n",
                "    col(\"ClientName\").cast(StringType()),\n",
                "    col(\"InsurerName\").cast(StringType()),\n",
                "    col(\"InsurerCountry\").cast(StringType()),\n",
                "    col(\"BrokerageUsd\").cast(DoubleType()),\n",
                "    col(\"PremiumUsd\").cast(DoubleType()),\n",
                "    col(\"SystemProductId\").cast(StringType()),\n",
                "    col(\"PartyIdWtw\").cast(StringType()),\n",
                "    col(\"DunsNumber\").cast(StringType()),\n",
                "    col(\"TransactionType\").cast(StringType()),\n",
                "    col(\"InsuredID\").cast(StringType()).alias(\"SystemId\"),\n",
                "    col(\"DataSource\").cast(StringType()),\n",
                "    col(\"ClientId\").cast(StringType()).alias(\"ClientIdWtw\"),\n",
                "    col(\"RevenueCountry\").cast(StringType()),\n",
                "    col(\"FinalDate\").cast(DateType()),\n",
                "    col(\"PolicyDescription\").cast(StringType()),\n",
                "    col(\"ReinsuranceDescription\").cast(StringType()),\n",
                "    col(\"MAPPED_INSURER\").cast(StringType()).alias(\"InsurerMapping\"),\n",
                "    col(\"`Lloyd's Asia or Lloyd's London`\").cast(StringType()).alias(\"Lloyds\"),\n",
                "    col(\"`Level 2 Mapping`\").cast(StringType()).alias(\"SubProductClass\"),\n",
                "    col(\"GLOBS\").cast(StringType()).alias(\"Globs\"),\n",
                "    col(\"`GLOBS SPLIT P&C`\").cast(StringType()).alias(\"GlobsSplitPnc\")\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Write to Silver\n",
                "# -----------------------\n",
                "target_table = \"APAC_Reporting_LH.clean_eclipse_chloe\"\n",
                "\n",
                "print(f\"Writing to {target_table}...\")\n",
                "df_enriched.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(target_table) \n",
                "\n",
                "print(f\"Success. Rows Processed: {df_enriched.count()}\")\n",
                "display(df_enriched.limit(10))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
