{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eclipse Silver Notebook\n",
    "Migration of Alteryx Eclipse workflow to Fabric PySpark.\n",
    "- **Asia Stream**: `src_eclipse_crb`\n",
    "- **London Stream**: `src_eclipse_london`\n",
    "- **Output**: `clean_eclipse` in `APAC_Reporting_LH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Configuration\n",
    "# -----------------------------\n",
    "from pyspark.sql.functions import col, when, trim, upper, lit, current_date, coalesce, isnan, count, concat, expr, size, collect_set, regexp_replace\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType, DateType, StringType, LongType, FloatType, DecimalType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import re\n",
    "\n",
    "# Helper: Safe Casting\n",
    "# --- APPLY CLEANSING TRANSFORMATIONS ---\n",
    "print(\"Applying cleansing transformations...\")\n",
    "\n",
    "def cleanse_dataframe(df):\n",
    "    print(\"Applying cleansing transformations...\")\n",
    "    \n",
    "    string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, (IntegerType, DoubleType, LongType, FloatType, DecimalType))]\n",
    "\n",
    "    cleansed_df = df\n",
    "\n",
    "    for col_name in numeric_cols:\n",
    "        cleansed_df = cleansed_df.withColumn(\n",
    "            col_name,\n",
    "            F.coalesce(F.col(f\"`{col_name}`\"), F.lit(0))\n",
    "        )\n",
    "\n",
    "    for col_name in string_cols:\n",
    "        cleansed_df = cleansed_df.withColumn(\n",
    "            col_name,\n",
    "            F.upper(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.trim(\n",
    "                            F.coalesce(F.col(f\"`{col_name}`\"), F.lit(''))\n",
    "                        ),\n",
    "                        r'[\\t\\n\\r]', ''\n",
    "                    ),\n",
    "                    r'\\s+', ' '\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if 'Likelihood_of_Win' in cleansed_df.columns:\n",
    "        cleansed_df = cleansed_df.withColumn(\n",
    "            \"Likelihood_of_Win\",\n",
    "            F.regexp_replace(F.col(\"`Likelihood_of_Win`\"), \"%\", \"\").cast(DoubleType())\n",
    "        )\n",
    "\n",
    "    print(\"Cleansing finished.\")\n",
    "    return cleansed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Bronze Data\n",
    "# ------------------------\n",
    "# 1. Asia Stream (Eclipse Recurring Report)\n",
    "try:\n",
    "    # UPDATED: Using SQL Endpoint access as per standard\n",
    "    df_asia = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.src_eclipse_crb\")\n",
    "    df_asia = df_asia.withColumn(\"Origin_Stream\", lit(\"Asia\")) \\\n",
    "        .filter(~col(\"LegalEntity\").isin(\"PT. Willis Reinsurance Brokers Indonesia\", \"Willis Towers Watson Taiwan Limited\"))\n",
    "except AnalysisException:\n",
    "    print(\"WARNING: src_eclipse_crb not found in APAC_CRM_Analytics_LH. Creating empty dummy DF.\")\n",
    "    df_asia = spark.createDataFrame([], schema=\"LegalEntity string, BusinessUnit string, Team string, ClassOfBusiness string, UW string\")\n",
    "\n",
    "# 2. London Stream (Combined MIR files)\n",
    "try:\n",
    "    # UPDATED: Using SQL Endpoint access as per standard\n",
    "    df_london = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.src_eclipse_london\")\n",
    "    df_london = df_london.withColumn(\"Origin_Stream\", lit(\"London\"))\n",
    "\n",
    "    # London Filters (Source.Name based logic)\n",
    "    # Lists for exclusion based on filename context\n",
    "    exclude_12046 = [\"China\", \"Hong Kong\", \"India\", \"Japan\", \"Malaysia\", \"New Zealand\", \"Philippines\", \"Republic of Korea\", \"Singapore\", \"Taiwan\", \"Thailand\"]\n",
    "    exclude_12047 = [\"Bahrain\", \"Cyprus\", \"Georgia\", \"Jordan\", \"Kazakhstan\", \"Kuwait\", \"Oman\", \"Qatar\", \"Turkey\", \"United Arab Emirates\"]\n",
    "\n",
    "    # Condition: (Contains 12046 AND Country in Exclude List A) OR (Contains 12047 AND Country NOT in Exclude List B)\n",
    "    # Using backticks for Source.Name to handle the dot\n",
    "    cond_exclude = (\n",
    "        (col(\"`Source.Name`\").contains(\"12046\") & col(\"UWCountry\").isin(exclude_12046)) | \n",
    "        (col(\"`Source.Name`\").contains(\"12047\") & ~col(\"UWCountry\").isin(exclude_12047))\n",
    "    )\n",
    "    df_london = df_london.filter(~cond_exclude)\n",
    "except AnalysisException:\n",
    "    print(\"WARNING: src_eclipse_london not found in APAC_CRM_Analytics_LH. Creating empty dummy DF.\")\n",
    "    df_london = spark.createDataFrame([], schema=\"LegalEntity string, BusinessUnit string, Team string, ClassOfBusiness string, UW string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Transformation Logic (Replicating Alteryx Tools 93 & 134)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# --- ASIA LOGIC (Alteryx Tool 93) ---\n",
    "# REVENUE COUNTRY Logic:\n",
    "cond_rev_country_asia = (\n",
    "    when(col(\"BU + TEAM\") == \"Retail+Commercial\", \"Singapore\")\n",
    "    .when(col(\"BU + TEAM\") == \"Retail+Construction\", \"Singapore\")\n",
    "    .when(col(\"BU + TEAM\").contains(\"Retail+Client Service Team\"), \"Singapore\")\n",
    "    .when(col(\"LegalEntity\").contains(\"Hong Kong\"), \"Hong Kong\")\n",
    "    .when(col(\"LegalEntity\").contains(\"Philippines\"), \"Philippines\")\n",
    "    .otherwise(\"Regional Specialism\")\n",
    ")\n",
    "\n",
    "df_asia_trans = df_asia \\\n",
    "    .withColumn(\"DATA SOURCE\", lit(\"Eclipse\")) \\\n",
    "    .withColumn(\"CLIENT ID\", coalesce(col(\"Willis Party ID\"), col(\"InsuredID\"))) \\\n",
    "    .withColumn(\"BU + TEAM\", concat(col(\"BusinessUnit\"), lit(\"+\"), col(\"Team\"))) \\\n",
    "    .withColumn(\"REVENUE COUNTRY\", cond_rev_country_asia) \\\n",
    "    .withColumn(\"PRODUCTS TO BE MAPPED\", upper(concat(col(\"BusinessUnit\"), lit(\"+\"), col(\"Team\"), lit(\"+\"), col(\"ClassOfBusiness\")))) \\\n",
    "    .withColumn(\"UW_CLEAN\", upper(trim(col(\"UW\")))) \\\n",
    "    .withColumn(\"FINAL DATE\", when(col(\"TransRef\") == \"INVOICE DATE\", col(\"CreatedDate\")).otherwise(col(\"InceptionDate\"))) \\\n",
    "    .withColumn(\"POLICY DESCRIPTION\", when(col(\"BusinessType\") == \"Reinsurance\", lit(\"Reinsurance\")).otherwise(lit(\"null\"))) \\\n",
    "    .withColumn(\"REINSURANCE DESCRIPTION\", lit(\"null\"))\n",
    "\n",
    "# --- LONDON LOGIC (Alteryx Tool 134) ---\n",
    "df_london_trans = df_london \\\n",
    "    .withColumn(\"DATA SOURCE\", lit(\"Eclipse\")) \\\n",
    "    .withColumn(\"CLIENT ID\", coalesce(col(\"Willis Party ID\"), col(\"InsuredID\"))) \\\n",
    "    .withColumn(\"REVENUE COUNTRY\", lit(\"London Placements\")) \\\n",
    "    .withColumn(\"PRODUCTS TO BE MAPPED\", trim(upper(col(\"ClassOfBusiness\")))) \\\n",
    "    .withColumn(\"UW_CLEAN\", upper(trim(col(\"UW\")))) \\\n",
    "    .withColumn(\"FINAL DATE\", when(col(\"TransRef\") == \"INVOICE DATE\", col(\"CreatedDate\")).otherwise(col(\"InceptionDate\"))) \\\n",
    "    .withColumn(\"POLICY DESCRIPTION\", when(col(\"BusinessType\") == \"Reinsurance\", lit(\"Reinsurance\")).otherwise(lit(\"null\"))) \\\n",
    "    .withColumn(\"REINSURANCE DESCRIPTION\", lit(\"null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Union & Unification (Replicating Alteryx Union)\n",
    "# ------------------------------------------------------\n",
    "# Standardize columns before union if needed, or rely on allowMissingColumns\n",
    "df_unified = df_asia_trans.unionByName(df_london_trans, allowMissingColumns=True)\n",
    "\n",
    "# Common Final Transform (Alteryx Select Tool 131 Rename)\n",
    "# We rename columns first to matches the target schema before cleaning and joining\n",
    "\n",
    "# FIX: Drop existing 'DEPARTMENT' column if it exists to avoid ambiguity when renaming 'Team'\n",
    "# Spark's drop is case-insensitive usually, but being explicit.\n",
    "for c in [col_name for col_name in df_unified.columns if col_name.lower() == \"department\"]:\n",
    "    df_unified = df_unified.drop(c)\n",
    "\n",
    "df_renamed = df_unified \\\n",
    "    .withColumnRenamed(\"Team\", \"DEPARTMENT\") \\\n",
    "    .withColumnRenamed(\"PolicyRef\", \"INVOICE/POLICY NUMBER\") \\\n",
    "    .withColumnRenamed(\"BusinessType\", \"BUSINESS TYPE\") \\\n",
    "    .withColumnRenamed(\"InceptionDate\", \"INCEPTION DATE\") \\\n",
    "    .withColumnRenamed(\"ExpiryDate\", \"EXPIRY DATE\") \\\n",
    "    .withColumnRenamed(\"Account Handler\", \"ACCOUNT HANDLER\") \\\n",
    "    .withColumnRenamed(\"CreatedDate\", \"INVOICE DATE\") \\\n",
    "    .withColumnRenamed(\"Insured\", \"CLIENT NAME\") \\\n",
    "    .withColumnRenamed(\"UW\", \"INSURER NAME\") \\\n",
    "    .withColumnRenamed(\"UWCountry\", \"INSURER COUNTRY\") \\\n",
    "    .withColumnRenamed(\"GrossBkgeUSDPlan\", \"BROKERAGE (USD)\") \\\n",
    "    .withColumnRenamed(\"GrossPremNonTtyUSDPlan\", \"PREMIUM (USD)\") \\\n",
    "    .withColumnRenamed(\"ClassOfBusiness\", \"SYSTEM PRODUCT ID\") \\\n",
    "    .withColumnRenamed(\"Willis Party ID\", \"PARTY ID (WTW)\") \\\n",
    "    .withColumnRenamed(\"Dun and Bradstreet No\", \"DUNS NUMBER\") \\\n",
    "    .withColumnRenamed(\"Revenue Type\", \"TRANSACTION TYPE\")\n",
    "\n",
    "# --- APPLY CLEANSING TRANSFORMATIONS ---\n",
    "# Apply the robust cleansing function here (Replaces nulls, trims, uppercases strings)\n",
    "df_cleansed = cleanse_dataframe(df_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Reference Joins (Replicating Tools 101, 102, 139, 140)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# 1. Load Reference Tables\n",
    "try:\n",
    "    # UPDATED: Using SQL Endpoint access as per standard, correct table names\n",
    "    ref_insurer = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.ref_Chloe_insurer_mapping\")\n",
    "    ref_product = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.ref_Chloe_eclipse_product_mapping\")\n",
    "    ref_trans = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.ref_Chloe_Transaction_type_mapping\")\n",
    "except:\n",
    "    print(\"Reference tables missing (Insurer/Product/TransType). Skipping specific joins (Mock Mode).\")\n",
    "    ref_insurer = None\n",
    "    ref_product = None\n",
    "    ref_trans = None\n",
    "\n",
    "\n",
    "# 2. Join Insurer\n",
    "if ref_insurer:\n",
    "    # Join on INSURER NAME == Insurer (Robust: Trim + Upper)\n",
    "    df_joined_1 = df_cleansed.join(\n",
    "        ref_insurer, \n",
    "        F.trim(F.upper(df_cleansed[\"INSURER NAME\"])) == F.trim(F.upper(ref_insurer[\"Insurer\"])), \n",
    "        \"left\"\n",
    "    )\n",
    "else:\n",
    "    df_joined_1 = df_cleansed\n",
    "\n",
    "# 3. Join Product\n",
    "if ref_product:\n",
    "    # Join on PRODUCTS TO BE MAPPED == Filter Fac Product (Robust: Trim + Upper)\n",
    "    df_joined_2 = df_joined_1.join(\n",
    "        ref_product, \n",
    "        F.trim(F.upper(df_joined_1[\"PRODUCTS TO BE MAPPED\"])) == F.trim(F.upper(ref_product[\"Filter Fac Product\"])), \n",
    "        \"left\"\n",
    "    )\n",
    "else:\n",
    "    df_joined_2 = df_joined_1\n",
    "\n",
    "# 4. Join Transaction Type\n",
    "if ref_trans:\n",
    "    # Join on TransType == TransType (Robust: Trim + Upper)\n",
    "    df_joined_3 = df_joined_2.join(\n",
    "        ref_trans, \n",
    "        F.trim(F.upper(df_joined_2[\"TransType\"])) == F.trim(F.upper(ref_trans[\"TransType\"])), \n",
    "        \"left\"\n",
    "    )\n",
    "else:\n",
    "    df_joined_3 = df_joined_2\n",
    "\n",
    "# Set final dataframe for output\n",
    "df_enriched = df_joined_3.select(\n",
    "    col(\"DEPARTMENT\"),\n",
    "    col(\"INVOICE/POLICY NUMBER\"),\n",
    "    col(\"BUSINESS TYPE\"),\n",
    "    col(\"INCEPTION DATE\"),\n",
    "    col(\"EXPIRY DATE\"),\n",
    "    col(\"ACCOUNT HANDLER\"),\n",
    "    col(\"INVOICE DATE\"),\n",
    "    col(\"CLIENT NAME\"),\n",
    "    col(\"INSURER NAME\"),\n",
    "    col(\"INSURER COUNTRY\"),\n",
    "    col(\"BROKERAGE (USD)\"),\n",
    "    col(\"PREMIUM (USD)\"),\n",
    "    col(\"NetClientPremNonTtyUSDPlan\"),\n",
    "    col(\"SYSTEM PRODUCT ID\"),\n",
    "    col(\"PARTY ID (WTW)\"),\n",
    "    col(\"DUNS NUMBER\"),\n",
    "    col(\"TRANSACTION TYPE\"),\n",
    "    col(\"`Effective Date`\"),\n",
    "    col(\"InsuredID\").alias(\"SYSTEM ID\"),\n",
    "    col(\"DATA SOURCE\"),\n",
    "    col(\"CLIENT ID\").alias(\"CLIENT ID (WTW)\"),\n",
    "    col(\"REVENUE COUNTRY\"),\n",
    "    col(\"FINAL DATE\"),\n",
    "    col(\"POLICY DESCRIPTION\"),\n",
    "    col(\"REINSURANCE DESCRIPTION\"),\n",
    "    col(\"PRODUCTS TO BE MAPPED\"),\n",
    "    col(\"MAPPED_INSURER\").alias(\"INSURER MAPPING\"),\n",
    "    col(\"Lloyd's Asia or Lloyd's London\").alias(\"LLOYDS\"),\n",
    "    col(\"Level 2 Mapping\").alias(\"SUB PRODUCT CLASS\"),\n",
    "    col(\"GLOBS\"),\n",
    "    col(\"GLOBS SPLIT P&C\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Write to Silver\n",
    "# -----------------------\n",
    "target_table = \"APAC_Reporting_LH.clean_eclipse\"\n",
    "\n",
    "print(f\"Writing to {target_table}...\")\n",
    "df_enriched.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"clean_eclipse\") \n",
    "\n",
    "print(f\"Success. Rows Processed: {df_enriched.count()}\")\n",
    "display(df_enriched.limit(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
