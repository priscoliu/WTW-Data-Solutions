{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Arias Silver Notebook\n",
                "Migration of Alteryx Arias workflow to Fabric PySpark.\n",
                "- **Source**: `src_arias_crb` (Japanese columns)\n",
                "- **Output**: `clean_arias` in `APAC_Reporting_LH`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Setup & Configuration\n",
                "# -----------------------------\n",
                "from pyspark.sql.functions import col, when, trim, upper, lit, current_date, coalesce, isnan, count, concat, expr, size, collect_set, regexp_replace, to_date, year\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql.types import IntegerType, DoubleType, DateType, StringType, LongType, FloatType, DecimalType\n",
                "from pyspark.sql.utils import AnalysisException\n",
                "\n",
                "# Helper: Cleanse DataFrame\n",
                "print(\"Applying cleansing transformations...\")\n",
                "\n",
                "def cleanse_dataframe(df):\n",
                "    print(\"Applying cleansing transformations...\")\n",
                "    \n",
                "    string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
                "    numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, (IntegerType, DoubleType, LongType, FloatType, DecimalType))]\n",
                "\n",
                "    cleansed_df = df\n",
                "\n",
                "    for col_name in numeric_cols:\n",
                "        cleansed_df = cleansed_df.withColumn(\n",
                "            col_name,\n",
                "            F.coalesce(F.col(f\"`{col_name}`\"), F.lit(0))\n",
                "        )\n",
                "\n",
                "    for col_name in string_cols:\n",
                "        cleansed_df = cleansed_df.withColumn(\n",
                "            col_name,\n",
                "            F.upper(\n",
                "                F.regexp_replace(\n",
                "                    F.regexp_replace(\n",
                "                        F.trim(\n",
                "                            F.coalesce(F.col(f\"`{col_name}`\"), F.lit(''))\n",
                "                        ),\n",
                "                        r'[\\t\\n\\r]', ''\n",
                "                    ),\n",
                "                    r'\\s+', ' '\n",
                "                )\n",
                "            )\n",
                "        )\n",
                "\n",
                "    print(\"Cleansing finished.\")\n",
                "    return cleansed_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02a60c04",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Load Bronze Data & Rename Columns\n",
                "# -----------------------------------------\n",
                "try:\n",
                "    # Load src_arias_crb\n",
                "    df_arias = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.src_arias_crb\")\n",
                "    \n",
                "    # DEBUG: Print source schema and sample data to understand types\n",
                "    print(\"=== SOURCE SCHEMA ===\")\n",
                "    df_arias.printSchema()\n",
                "    print(\"\\n=== SOURCE COLUMNS ===\")\n",
                "    print(df_arias.columns)\n",
                "    print(\"\\n=== SAMPLE DATA (first 3 rows) ===\")\n",
                "    display(df_arias.limit(3))\n",
                "    \n",
                "    # Rename Japanese columns to English\n",
                "    df_arias_renamed = df_arias \\\n",
                "        .withColumnRenamed(\"請求書番号\", \"Invoice No.\") \\\n",
                "        .withColumnRenamed(\"保険始期\", \"From\") \\\n",
                "        .withColumnRenamed(\"保険終期\", \"To\") \\\n",
                "        .withColumnRenamed(\"契約者名\", \"Name of Client\") \\\n",
                "        .withColumnRenamed(\"保険会社名\", \"Insurer\") \\\n",
                "        .withColumnRenamed(\"保険種類\", \"Class of Insurance\") \\\n",
                "        .withColumnRenamed(\"保険料\", \"Premium\") \\\n",
                "        .withColumnRenamed(\"手数料（税抜_\", \"Full Commission\") \\\n",
                "        .withColumnRenamed(\"収益認識日\", \"Premium Receipt/Paid Date\") \\\n",
                "        .withColumnRenamed(\"Policy No\", \"Policy No.\") \\\n",
                "        .withColumnRenamed(\"チーム名\", \"Team\") \\\n",
                "        .withColumnRenamed(\"ＡＥ名\", \"A/E\") \\\n",
                "        .withColumnRenamed(\"Recurring\", \"Recurring/Non-Recurring\") \\\n",
                "        .withColumnRenamed(\"６分類\", \"6分類\")\n",
                "    \n",
                "    # DEBUG: Print renamed schema to verify types\n",
                "    print(\"\\n=== RENAMED SCHEMA ===\")\n",
                "    df_arias_renamed.printSchema()\n",
                "\n",
                "except AnalysisException:\n",
                "    print(\"WARNING: src_arias_crb not found in APAC_CRM_Analytics_LH. Creating empty dummy DF.\")\n",
                "    df_arias_renamed = spark.createDataFrame([], schema=\"`Invoice No.` string, `From` string, `To` string, `Name of Client` string, `Insurer` string, `Class of Insurance` string, `Premium` double, `Full Commission` double, `Premium Receipt/Paid Date` string, `Policy No.` string, `Team` string, `A/E` string, `Recurring/Non-Recurring` string, `6分類` string\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af2db384",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Transformation Logic (Alteryx Tool 147 - Arias Data Formula)\n",
                "# --------------------------------------------------------------------\n",
                "\n",
                "# 0. Force correct types BEFORE any transformation\n",
                "# Premium and Full Commission may come in as strings from Excel source\n",
                "df_typed = df_arias_renamed \\\n",
                "    .withColumn(\"Premium\", col(\"Premium\").cast(DoubleType())) \\\n",
                "    .withColumn(\"Full Commission\", col(\"`Full Commission`\").cast(DoubleType()))\n",
                "\n",
                "# 1. Date Conversion\n",
                "# Source dates may be: yyyyMMdd numeric, yyyy-MM-dd string, or already DateType\n",
                "# Try multiple formats with coalesce for robustness\n",
                "df_transformed = df_typed \\\n",
                "    .withColumn(\"InvoiceDate\", coalesce(\n",
                "        to_date(col(\"Premium Receipt/Paid Date\").cast(StringType()), \"yyyyMMdd\"),\n",
                "        to_date(col(\"Premium Receipt/Paid Date\").cast(StringType()), \"yyyy-MM-dd\"),\n",
                "        col(\"Premium Receipt/Paid Date\").cast(DateType())\n",
                "    )) \\\n",
                "    .withColumn(\"InceptionDate\", coalesce(\n",
                "        to_date(col(\"From\").cast(StringType()), \"yyyyMMdd\"),\n",
                "        to_date(col(\"From\").cast(StringType()), \"yyyy-MM-dd\"),\n",
                "        col(\"From\").cast(DateType())\n",
                "    )) \\\n",
                "    .withColumn(\"ExpiryDate\", coalesce(\n",
                "        to_date(col(\"To\").cast(StringType()), \"yyyyMMdd\"),\n",
                "        to_date(col(\"To\").cast(StringType()), \"yyyy-MM-dd\"),\n",
                "        col(\"To\").cast(DateType())\n",
                "    )) \\\n",
                "    .withColumn(\"FinalDate\", col(\"InceptionDate\"))\n",
                "\n",
                "# DEBUG: Check date parsing results\n",
                "print(\"=== DATE PARSING CHECK ===\")\n",
                "df_transformed.select(\n",
                "    col(\"Premium Receipt/Paid Date\"), col(\"InvoiceDate\"),\n",
                "    col(\"From\"), col(\"InceptionDate\"),\n",
                "    col(\"To\"), col(\"ExpiryDate\"),\n",
                "    col(\"Premium\"), col(\"Full Commission\")\n",
                ").show(5, truncate=False)\n",
                "\n",
                "# 2. Hardcoded Fields (Alteryx Formula Tool 147)\n",
                "df_transformed = df_transformed \\\n",
                "    .withColumn(\"DataSource\", lit(\"Arias\")) \\\n",
                "    .withColumn(\"RevenueCountry\", lit(\"Japan\")) \\\n",
                "    .withColumn(\"Segment\", lit(\"null\")) \\\n",
                "    .withColumn(\"InsurerCountry\", lit(\"JAPAN\")) \\\n",
                "    .withColumn(\"DunsNumber\", lit(\"UNKNOWN-ARIAS\")) \\\n",
                "    .withColumn(\"BusinessType\", lit(\"UNKNOWN\")) \\\n",
                "    .withColumn(\"PartyIdWtw\", lit(\"UNKNOWN-ARIAS\")) \\\n",
                "    .withColumn(\"Ccy\", lit(\"JPY\")) \\\n",
                "    .withColumn(\"ReinsuranceDescription\", lit(\"null\")) \\\n",
                "    .withColumn(\"PolicyDescription\", lit(\"null\")) \\\n",
                "    .withColumn(\"Department\", lit(\"null\")) \\\n",
                "    .withColumn(\"TransactionType\",\n",
                "        when(col(\"`Recurring/Non-Recurring`\") == \"R\", lit(\"RENEWAL\"))\n",
                "        .when(col(\"`Recurring/Non-Recurring`\") == \"N\", lit(\"NEW\"))\n",
                "        .otherwise(col(\"`Recurring/Non-Recurring`\"))\n",
                "    )\n",
                "\n",
                "# 3. Derived Fields (Alteryx Formula Tool 147)\n",
                "# CLIENT ID = [Name of Client]\n",
                "# SYSTEM ID = [CLIENT ID] = [Name of Client]\n",
                "# INCEPTION YEAR = DateTimeYear([INCEPTION DATE])\n",
                "# CCYYEAR = \"JPY\" + \"-\" + [INCEPTION YEAR]\n",
                "df_transformed = df_transformed \\\n",
                "    .withColumn(\"ClientIdWtw\", col(\"Name of Client\")) \\\n",
                "    .withColumn(\"SystemId\", col(\"Name of Client\")) \\\n",
                "    .withColumn(\"InceptionYear\", year(col(\"InceptionDate\")).cast(StringType())) \\\n",
                "    .withColumn(\"Ccyyear\", concat(lit(\"JPY-\"), col(\"InceptionYear\")))\n",
                "\n",
                "# 4. Rename remaining columns\n",
                "df_transformed = df_transformed \\\n",
                "    .withColumnRenamed(\"Name of Client\", \"ClientName\") \\\n",
                "    .withColumnRenamed(\"Insurer\", \"InsurerName\") \\\n",
                "    .withColumnRenamed(\"Class of Insurance\", \"SystemProductId\") \\\n",
                "    .withColumnRenamed(\"A/E\", \"AccountHandler\") \\\n",
                "    .withColumnRenamed(\"Policy No.\", \"InvoicePolicyNumber\")\n",
                "\n",
                "# 5. Cleansing (before joins to ensure matches)\n",
                "df_cleansed = cleanse_dataframe(df_transformed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Reference Joins\n",
                "# -----------------------\n",
                "\n",
                "# 1. Load Reference Tables\n",
                "try:\n",
                "    ref_currency = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.ref_Chloe_asia_currency_mapping\")\n",
                "    ref_product = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.ref_Chloe_arias_product_mapping\")\n",
                "    ref_insurer = spark.sql(\"SELECT * FROM APAC_CRM_Analytics_LH.ref_Chloe_insurer_mapping\")\n",
                "except:\n",
                "    print(\"Reference tables missing. Skipping joins (Mock Mode).\")\n",
                "    ref_currency = None\n",
                "    ref_product = None\n",
                "    ref_insurer = None\n",
                "\n",
                "# 2. Join Currency (on Ccyyear)\n",
                "if ref_currency:\n",
                "    df_joined_1 = df_cleansed.join(\n",
                "        ref_currency, \n",
                "        df_cleansed[\"Ccyyear\"] == ref_currency[\"CCYYEAR\"], \n",
                "        \"left\"\n",
                "    ).select(df_cleansed[\"*\"], ref_currency[\"Value\"].alias(\"Ccyvalue\"))\n",
                "else:\n",
                "    df_joined_1 = df_cleansed.withColumn(\"Ccyvalue\", lit(1.0))\n",
                "\n",
                "# 3. Join Product (on SystemProductId == Class of Insurance)\n",
                "if ref_product:\n",
                "    df_joined_2 = df_joined_1.join(\n",
                "        ref_product, \n",
                "        F.trim(F.upper(df_joined_1[\"SystemProductId\"])) == F.trim(F.upper(ref_product[\"Class of Insurance\"])), \n",
                "        \"left\"\n",
                "    ).select(\n",
                "        df_joined_1[\"*\"], \n",
                "        ref_product[\"Lvl 2 Mapping\"].alias(\"SubProductClass\"),\n",
                "        ref_product[\"GLOBs\"].alias(\"Globs\"),\n",
                "        ref_product[\"GLOBS SPLIT P&C\"].alias(\"GlobsSplitPnc\")\n",
                "    )\n",
                "else:\n",
                "    df_joined_2 = df_joined_1 \\\n",
                "        .withColumn(\"SubProductClass\", lit(None)) \\\n",
                "        .withColumn(\"Globs\", lit(None)) \\\n",
                "        .withColumn(\"GlobsSplitPnc\", lit(None))\n",
                "\n",
                "# 4. Join Insurer (on InsurerName == Insurer)\n",
                "if ref_insurer:\n",
                "    df_joined_3 = df_joined_2.join(\n",
                "        ref_insurer, \n",
                "        F.trim(F.upper(df_joined_2[\"InsurerName\"])) == F.trim(F.upper(ref_insurer[\"Insurer\"])), \n",
                "        \"left\"\n",
                "    ).select(\n",
                "        df_joined_2[\"*\"], \n",
                "        ref_insurer[\"MAPPED_INSURER\"].alias(\"InsurerMapping\"),\n",
                "        ref_insurer[\"LLOYDS\"].alias(\"Lloyds\"),\n",
                "        ref_insurer[\"Lloyd's Asia or Lloyd's London\"].alias(\"LloydsAsiaOrLondon\") if \"Lloyd's Asia or Lloyd's London\" in ref_insurer.columns else lit(None).alias(\"LloydsAsiaOrLondon\")\n",
                "    )\n",
                "else:\n",
                "    df_joined_3 = df_joined_2 \\\n",
                "        .withColumn(\"InsurerMapping\", lit(None)) \\\n",
                "        .withColumn(\"Lloyds\", lit(None))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Final Calculations & Selection\n",
                "# --------------------------------------\n",
                "\n",
                "df_calculated = df_joined_3 \\\n",
                "    .withColumn(\"PremiumUsd\", col(\"Premium\") * coalesce(col(\"Ccyvalue\"), lit(0))) \\\n",
                "    .withColumn(\"BrokerageUsd\", col(\"Full Commission\") * coalesce(col(\"Ccyvalue\"), lit(0)))\n",
                "\n",
                "# Select Final Columns matching target schema (PascalCase, no spaces)\n",
                "df_final = df_calculated.select(\n",
                "    col(\"ClientName\").cast(StringType()),\n",
                "    col(\"InsurerName\").cast(StringType()),\n",
                "    col(\"SystemProductId\").cast(StringType()),\n",
                "    col(\"AccountHandler\").cast(StringType()),\n",
                "    col(\"InvoicePolicyNumber\").cast(StringType()),\n",
                "    col(\"TransactionType\").cast(StringType()),\n",
                "    col(\"InvoiceDate\").cast(DateType()),\n",
                "    col(\"InceptionDate\").cast(DateType()),\n",
                "    col(\"ExpiryDate\").cast(DateType()),\n",
                "    col(\"DataSource\").cast(StringType()),\n",
                "    col(\"RevenueCountry\").cast(StringType()),\n",
                "    col(\"Department\").cast(StringType()),\n",
                "    col(\"ClientIdWtw\").cast(StringType()),\n",
                "    col(\"InsurerCountry\").cast(StringType()),\n",
                "    col(\"DunsNumber\").cast(StringType()),\n",
                "    col(\"BusinessType\").cast(StringType()),\n",
                "    col(\"SystemId\").cast(StringType()),\n",
                "    col(\"PartyIdWtw\").cast(StringType()),\n",
                "    col(\"ReinsuranceDescription\").cast(StringType()),\n",
                "    col(\"PolicyDescription\").cast(StringType()),\n",
                "    col(\"SubProductClass\").cast(StringType()),\n",
                "    col(\"Globs\").cast(StringType()),\n",
                "    col(\"GlobsSplitPnc\").cast(StringType()),\n",
                "    col(\"InsurerMapping\").cast(StringType()),\n",
                "    col(\"Lloyds\").cast(StringType()),\n",
                "    col(\"PremiumUsd\").cast(DoubleType()),\n",
                "    col(\"BrokerageUsd\").cast(DoubleType()),\n",
                "    col(\"FinalDate\").cast(DateType()),\n",
                "    col(\"Segment\").cast(StringType())\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Write to Silver\n",
                "# -----------------------\n",
                "target_table = \"APAC_Reporting_LH.clean_arias\"\n",
                "\n",
                "print(f\"Writing to {target_table}...\")\n",
                "df_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(target_table) \n",
                "\n",
                "print(f\"Success. Rows Processed: {df_final.count()}\")\n",
                "display(df_final.limit(10))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}